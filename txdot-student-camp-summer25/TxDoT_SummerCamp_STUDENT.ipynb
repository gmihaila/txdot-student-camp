{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgyaqzEKEnE-"
   },
   "source": [
    "<p align=\"right\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/85/TxDOT_logo_as_of_2023.svg\" width=\"550\"></p>\n",
    "\n",
    "# TxDOT ‚ÄúRoad-Bot‚Äù Mission\n",
    "## üöó STUDENT TEMPLATE ‚Äì Car Model Recognition\n",
    "\n",
    "Welcome to the fill-in-the-blanks version of the **Car Model Recognition** notebook.\n",
    "\n",
    "**Goal:** Teach an AI to tell a **Suzuki Swift** from a **Suzuki WagonR**.  Think of Road-Bot as a student. We‚Äôll show it hundreds of photos labeled Swift or WagonR, and it‚Äôll learn to tell which is which‚Äîlike flash cards for cars.\n",
    "\n",
    "**Why?** TxDOT technicians use these counts to plan parking-lot expansions and anti-congestion measures.  \n",
    "\n",
    "You are now *Road-Bot Junior Engineers*‚Äîlet‚Äôs get to work!  \n",
    "\n",
    "\n",
    "<p align=\"right\"><img src=\"https://www.cartoq.com/wp-content/uploads/2023/10/Maruti-Suzuki-WagonR-vs-Maruti-Suzuki-SwiftA.jpg\" width=\"800\"></p>\n",
    "\n",
    "\n",
    "### üìù How to Use This Notebook\n",
    "\n",
    "üîß You‚Äôll be working through a series of simple, fill-in-the-blank tasks.\n",
    "\n",
    "Here‚Äôs how to stay on track:\n",
    "\n",
    "- Look for cells labeled **üöÄ STUDENT TASK** ‚Äî those are your action zones!\n",
    "- After each one, run the **‚úÖ QUICK CHECK** cell right below it to make sure everything‚Äôs working.\n",
    "- If you see the message  \n",
    "  `\"Feel free to tweak earlier sliders & re-run.\"`  \n",
    "  it means you're close ‚Äî just try adjusting some values and run again.\n",
    "\n",
    "‚úÖ When everything checks out, the final cell will celebrate with a big green **ALL DONE!** banner.\n",
    "\n",
    "> üí° **Stuck?** Don‚Äôt worry ‚Äî just unfold the hint right after the task for help.\n",
    "\n",
    "---\n",
    "\n",
    "**You're in control ‚Äî explore, test, and have fun. You've got this! üöóüí®**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hz2C509e4G9"
   },
   "source": [
    "---\n",
    "## üö¶ Mission Brief ‚Äì Become a ‚ÄúRoad-Bot‚Äù Engineer\n",
    "\n",
    "TxDOT (Texas Department of Transportation) is building an AI helper called **Road-Bot**.  \n",
    "Its first job: scan dash-cam footage and **count how many Suzuki Swifts vs. Suzuki WagonRs** are parked near schools.  \n",
    "Those counts feed into *real* planning decisions‚Äîlike how wide to make new pickup/drop-off lanes.\n",
    "\n",
    "To turn raw pictures into trustworthy numbers, we‚Äôll follow the same pipeline professional data-scientists use:\n",
    "\n",
    "| Step | What happens | Why it matters |\n",
    "|------|--------------|----------------|\n",
    "| **1. Setup** (you just click ‚ñ∂) | Install PyTorch & download the mini car-photo dataset. | Gives us tools + data in one place so everyone starts on equal footing. |\n",
    "| **2. Tune the Basics** (üöÄ TASK #1) | Choose batch size, learning rate, and number of epochs. | These dials control **how fast** and **how safely** the network learns. |\n",
    "| **3. Augment the Data** (üöÄ TASK #2) | Decide if we flip, rotate, or recolor training images. | Extra ‚Äúlooks‚Äù at each car teach the model to ignore camera angle & lighting. |\n",
    "| **4. Build DataLoaders** | Package the photos into mini-batches PyTorch can read. | Feeds the network efficiently‚Äîlike handing flash-cards in neat stacks. |\n",
    "| **5. Pick a CNN Size** (üöÄ TASK #3) | Tiny / Small / Medium model presets. | Balances accuracy vs. training time on a classroom laptop. |\n",
    "| **6. Set the optimiser** (üöÄ TASK #4) | Select Adam / RMSprop / SGD. | Controls *how* weights are updated; can speed up or steady training. |\n",
    "| **7. Train & Watch** | The loop prints loss & accuracy each epoch. | You‚Äôll *see* Road-Bot get better‚Äîoften within 30 seconds. |\n",
    "| **8. Quick Check #4** | If validation accuracy > 80 %, a big green **ALL DONE!** banner appears. | Confirms the model is good enough for TxDOT‚Äôs planning tool. |\n",
    "\n",
    "\n",
    "**Your role:** After each *üöÄ STUDENT TASK* cell, hit ‚ñ∂ on the *‚úÖ QUICK CHECK* right below it.  \n",
    "When you hit the green banner, you‚Äôve officially graduated to **Road-Bot Junior Engineer**. üåü\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFRb7pt3GQw5"
   },
   "outputs": [],
   "source": [
    "#@title ‚öôÔ∏è Setup ‚Äì RUN ONCE (‚âà2 min) ‚ñ∏ installs libraries ‚Ä¢ downloads data { display-mode:\"form\" }\n",
    "\"\"\"\n",
    "What this cell does\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚úì Installs PyTorch, torchvision, tqdm\n",
    "‚úì Downloads the ZIP (~90 MB) if not cached\n",
    "‚úì Unzips into /content/cars-wagonr-swift\n",
    "‚úì Defines global constants used by later cells\n",
    "\"\"\"\n",
    "\n",
    "# import urllib.request, zipfile, time, sys, os\n",
    "\n",
    "# DATASET_URL = \"https://www.dropbox.com/scl/fi/45gxs6ddi5h51to3bvpjx/cars-wagonr-swift.zip\"\n",
    "# ZIP_NAME    = \"cars-wagonr-swift.zip\"\n",
    "# EXTRACT_DIR = \"/content/data\"\n",
    "\n",
    "# # ‚îÄ‚îÄ tiny spinner so Colab feels alive ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# def _spinner(msg=\"Setting up‚Ä¶\"):\n",
    "#     for ch in \"|/-\\\\\":\n",
    "#         sys.stdout.write(f\"\\r{msg} {ch}\")\n",
    "#         sys.stdout.flush()\n",
    "#         time.sleep(0.08)\n",
    "\n",
    "try:\n",
    "    # 1Ô∏è‚É£ install packages\n",
    "    # _spinner(\"üîß Installing PyTorch & friends\")\n",
    "    !pip -q install torch torchvision tqdm\n",
    "\n",
    "    # 2Ô∏è‚É£ import libraries\n",
    "    # _spinner(\"üì¶ Importing libraries\")\n",
    "    import torch, numpy as np, PIL, matplotlib.pyplot as plt\n",
    "    import torchvision.transforms as transforms\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    # # 3Ô∏è‚É£ download + unzip dataset (once)\n",
    "    # if not os.path.exists(EXTRACT_DIR):\n",
    "    #     # _spinner(\"‚¨áÔ∏è  Downloading dataset (~90 MB)\")\n",
    "    #     urllib.request.urlretrieve(DATASET_URL, ZIP_NAME)\n",
    "    #     # _spinner(\"üìÇ Unzipping dataset\")\n",
    "    #     zipfile.ZipFile(ZIP_NAME).extractall(EXTRACT_DIR)\n",
    "\n",
    "    print(\"\\n‚úÖ Environment ready!  Move to the first üöÄ STUDENT TASK.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ùå Setup failed:\", e)\n",
    "    raise  # re-throw so users notice\n",
    "\n",
    "# ---------- fixed paths & constants ----------\n",
    "TRAIN_DATA_PATH      = \"data_full/train\"\n",
    "VALIDATION_DATA_PATH = \"data_full/validation\"\n",
    "TEST_DATA_PATH       = \"data_full/test\"\n",
    "\n",
    "WAGONEER_LABEL = \"wagonr\"\n",
    "SWIFT_LABEL    = \"swift\"\n",
    "IMAGE_SIZE     = (50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNGY6H1dk-XG"
   },
   "source": [
    "# üìò **How Road-Bot Learns**\n",
    "\n",
    "Think of Road-Bot as a student. Road-Bot learns by studying labeled photos‚Äîsome show Suzuki Swifts, others show WagonRs.  We‚Äôll show it hundreds of photos labeled Swift or WagonR, and it‚Äôll learn to tell which is which‚Äîlike flash cards for cars.\n",
    "\n",
    "Imagine showing Road-Bot 16 flash cards at a time. Each time it sees a batch of photos, it tries to guess the car type, gets corrected, and adjusts its ‚Äúbrain.‚Äù After each group, it adjusts how it ‚Äòthinks‚Äô to improve accuracy. We repeat that process over and over‚Äîeach full run through the whole dataset is one epoch.\n",
    "\n",
    "\n",
    "> üìù **What do these numbers mean?**  \n",
    "> In this step, you‚Äôll choose:\n",
    ">\n",
    "> ‚Ä¢ üì¶ **Batch Size** ‚Äì How many photos Road-Bot sees at once (like flipping flash cards).  (*Hint! 8‚Äì64 is laptop-friendly*).\n",
    ">  \n",
    "> ‚Ä¢ üß† **Learning Rate** ‚Äì How big each adjustment is after an error. (*Hint! 0.001 is safe*)\n",
    ">\n",
    "> ‚Ä¢ üîÅ **Epochs** ‚Äì How many times it goes through all the flash cards. *Hint! Choose between 5-10 to show visible progress without stalling the workshop.*\n",
    "\n",
    "\n",
    "**Pro tip:** Start with the default values below, then tweak them and see what changes!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Kn2OFGsKDM2"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ STUDENT TASK #1 ‚Äì Tune the basics { display-mode:\"form\" }\n",
    "def _hyper():\n",
    "  BATCH_SIZE = 8       #@param {type:\"slider\",  label:\"üì¶ Batch size\",   min:8, max:64, step:8}\n",
    "  LEARNING_RATE = 0.001 #@param {type:\"number\",  label:\"üöÄ Learning rate\", step:0.0005}\n",
    "  EPOCHS = 3            #@param {type:\"slider\",  label:\"üîÅ Epochs\",       min:3, max:20}\n",
    "  return BATCH_SIZE, LEARNING_RATE, EPOCHS\n",
    "\n",
    "BATCH_SIZE, LEARNING_RATE, EPOCHS = _hyper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3GBG-tEIBw5"
   },
   "outputs": [],
   "source": [
    "#@title ‚úÖ Quick Check #1 { display-mode:\"form\" }\n",
    "\n",
    "assert 8 <= BATCH_SIZE <= 64,       \"Try something between 8 and 64.\"\n",
    "assert 1e-5 <= LEARNING_RATE <= 1,  \"Learning rate looks off.\"\n",
    "assert 3 <= EPOCHS <= 30,           \"Pick 3-30 epochs.\"\n",
    "print(\"üéâ Looks good! Move to the next cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BQijXClm-NK"
   },
   "source": [
    "# üì∏ How Road-Bot Sees the World\n",
    "\n",
    "Before we teach Road-Bot, let‚Äôs **peek at the photos** it‚Äôll be learning from.\n",
    "\n",
    "We‚Äôve already downloaded a mini dataset of **Suzuki Swift** and **Suzuki WagonR** pictures. Each image is labeled so Road-Bot knows what it‚Äôs looking at while it learns.\n",
    "\n",
    "This step is like **flipping through a study guide before class starts** ‚Äî we want to see what kind of examples Road-Bot will use to learn the difference between the two car types.\n",
    "\n",
    "---\n",
    "\n",
    "üß≠ **What to look for:**\n",
    "\n",
    "- Do the **Swift and WagonR** cars have noticeable shape or size differences?  \n",
    "- Are the **photo angles** consistent or all over the place?  \n",
    "- Do some images have tricky conditions like shadows or parked cars?\n",
    "\n",
    "---\n",
    "\n",
    "üìò **Why this matters:**  \n",
    "The more you understand the data, the better you can **train, troubleshoot, and improve** Road-Bot later.  \n",
    "This is your first glimpse of what Road-Bot will ‚Äúsee‚Äù as it learns to drive its neural engine.\n",
    "\n",
    "> *Pro tip:* You can come back and re-run this preview cell anytime you want to inspect the data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKFTjvigMtEB"
   },
   "outputs": [],
   "source": [
    "#@title üëÄ Meet the data ‚Äì run to view a sample { display-mode:\"form\" }\n",
    "\n",
    "from pathlib import Path\n",
    "import random, matplotlib.pyplot as plt, PIL\n",
    "\n",
    "def _show_sample(root, cls, n=6):\n",
    "  files = list(Path(root/cls).glob(\"*\"))\n",
    "  plt.figure(figsize=(9,6))\n",
    "  for i,f in enumerate(random.sample(files, n)):\n",
    "      img = PIL.Image.open(f)\n",
    "      plt.subplot(2,3,i+1); plt.imshow(img); plt.axis(\"off\")\n",
    "  plt.tight_layout()\n",
    "\n",
    "_show_sample(Path(\"/content/data/train\"), \"swift\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia7jcGPKngHv"
   },
   "source": [
    "# üìà How Road-Bot Gets Better at Seeing\n",
    "\n",
    "Some photos are bright, some are dark. Some cars are angled left, others right.  \n",
    "To make Road-Bot more flexible, we can **augment the training images**‚Äîflipping, rotating, or adjusting the color just a bit.\n",
    "\n",
    "This is like showing flash cards from different angles or lighting so the model won‚Äôt get confused in the real world.\n",
    "\n",
    "---\n",
    "\n",
    "üéõÔ∏è **Choose an augmentation pack** to help Road-Bot:\n",
    "- Flip cars left or right (helps with angle variation)\n",
    "- Rotate or zoom slightly (simulates imperfect photos)\n",
    "- Change brightness or hue (trains Road-Bot for cloudy or sunny days)\n",
    "\n",
    "> üß™ Try different packs and see which one improves performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eed-N2SAMtGY"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ STUDENT TASK #2 ‚Äì Choose an augmentation pack { display-mode:\"form\" }\n",
    "aug_pack = \"None\"  #@param [\"None\", \"Flip Only\", \"Flip + Rotate\", \"Flip + ColorJitter\"]\n",
    "\n",
    "import torchvision.transforms as T\n",
    "if aug_pack == \"None\":\n",
    "    aug_list = []\n",
    "elif aug_pack == \"Flip Only\":\n",
    "    aug_list = [T.RandomHorizontalFlip()]\n",
    "elif aug_pack == \"Flip + Rotate\":\n",
    "    aug_list = [T.RandomHorizontalFlip(),\n",
    "                T.RandomRotation(20)]\n",
    "else:  # Flip + ColorJitter\n",
    "    aug_list = [T.RandomHorizontalFlip(),\n",
    "                T.RandomRotation(20),\n",
    "                T.ColorJitter(brightness=.2, contrast=.2, saturation=.2)]\n",
    "\n",
    "# üîí üèóÔ∏è BUILD DATASET & DATALOADERS  (no edits needed) { display-mode:\"form\" }\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Compose transforms ------------------\n",
    "train_tfms = T.Compose(aug_list + [                # ‚Üê set in STUDENT TASK #2\n",
    "    T.Resize(IMAGE_SIZE),                          # (150,150)\n",
    "    T.ToTensor()\n",
    "])\n",
    "test_tfms  = T.Compose([\n",
    "    T.Resize(IMAGE_SIZE),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# --- 2. Point to the folders -----------------\n",
    "train_ds = ImageFolder(Path(TRAIN_DATA_PATH),      transform=train_tfms)\n",
    "val_ds   = ImageFolder(Path(VALIDATION_DATA_PATH), transform=test_tfms)\n",
    "test_ds  = ImageFolder(Path(TEST_DATA_PATH),       transform=test_tfms)\n",
    "\n",
    "# --- 3. Wrap in DataLoaders ------------------\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# print(f\"üéâ DataLoaders ready ‚Äì \"\n",
    "#       f\"{len(train_loader)} train batches | \"\n",
    "#       f\"{len(val_loader)} val batches | \"\n",
    "#       f\"{len(test_loader)} test batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYB8sy2PMtIt"
   },
   "outputs": [],
   "source": [
    "#@title ‚úÖ Quick Check #2 { display-mode:\"form\" }\n",
    "\n",
    "import torchvision.transforms as T, PIL, random, matplotlib.pyplot as plt\n",
    "tmp = T.Compose(aug_list + [T.ToTensor()])\n",
    "\n",
    "sample_path = \"/content/data/train/swift\"\n",
    "img = PIL.Image.open(random.choice(list(Path(sample_path).glob(\"*\"))))\n",
    "plt.imshow(tmp(img).permute(1,2,0)); plt.title(\"Augmentation preview\"); plt.axis(\"off\")\n",
    "print(\"üéâ Looks like the transform works!  Move on ‚ûú\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh1H6r2Hnt1l"
   },
   "source": [
    "# üèãüèº‚Äç‚ôÄÔ∏è How Powerful Should Road-Bot‚Äôs Brain Be?\n",
    "\n",
    "A Convolutional Neural Network (CNN) is like Road-Bot‚Äôs brain.  \n",
    "You can choose a **Tiny**, **Small**, or **Medium** version.\n",
    "\n",
    "Think of it like car engines:\n",
    "\n",
    "- **Tiny (3 layers)** ‚Äì Fast and simple. Good for short races.\n",
    "- **Small (4 layers)** ‚Äì Balanced. Fast and smart.\n",
    "- **Medium (5 layers)** ‚Äì Powerful, but takes longer to train.\n",
    "\n",
    "> üèéÔ∏è Bigger isn‚Äôt always better‚Äîpick based on how much time you have and how complex the task is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0Bo75KtMtKh"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ STUDENT TASK #3 ‚Äì Select a CNN size { display-mode:\"form\" }\n",
    "model_size = \"Tiny (3 layers)\"  #@param [\"Tiny (3 layers)\", \"Small (4 layers)\", \"Medium (5 layers)\"]\n",
    "\n",
    "import torch.nn as nn, torch\n",
    "\n",
    "def make_net(choice):\n",
    "    if choice == \"Tiny (3 layers)\":\n",
    "        layers = [3,16,32]      # output chans per conv layer\n",
    "    elif choice == \"Small (4 layers)\":\n",
    "        layers = [3,16,32,64]\n",
    "    else:                        # Medium\n",
    "        layers = [3,16,32,64,128]\n",
    "\n",
    "    convs, chans = [], layers[0]\n",
    "    for out_ch in layers[1:]:\n",
    "        convs += [nn.Conv2d(chans, out_ch, 3, padding=1),\n",
    "                  nn.BatchNorm2d(out_ch),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(2)]\n",
    "        chans = out_ch\n",
    "    return nn.Sequential(*convs,\n",
    "                         nn.Flatten(),\n",
    "                         nn.Linear(chans * (IMAGE_SIZE[0]//(2**len(layers[1:])))**2, 2))\n",
    "\n",
    "net = make_net(model_size).to(\"cpu\")    # GPU later if available\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lp2nh3BOHaY"
   },
   "outputs": [],
   "source": [
    "#@title ‚úÖ Quick Check #3 { display-mode:\"form\" }\n",
    "\n",
    "param_cnt = sum(p.numel() for p in net.parameters())\n",
    "assert 10_000 < param_cnt < 5_000_000, \"Param count off ‚Äì did you change something by accident?\"\n",
    "print(f\"üéØ Network has {param_cnt:,} trainable parameters ‚Äì good to go!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpn--980oF5b"
   },
   "source": [
    "# üìñ How Road-Bot Learns from Mistakes\n",
    "\n",
    "Each time Road-Bot guesses wrong, it adjusts its weights to do better next time.  \n",
    "The **optimiser** controls *how* those adjustments happen‚Äîlike the gears in a race car‚Äôs transmission.\n",
    "\n",
    "Choose from:\n",
    "\n",
    "- ‚öôÔ∏è **SGD** ‚Äì The original, classic learning strategy.\n",
    "- ‚ö° **Adam** ‚Äì Fast and smooth. Great all-round choice.\n",
    "- üß≠ **RMSprop** ‚Äì Like Adam, but sometimes better for noisy data.\n",
    "\n",
    "> üö¶ Try one, then test again to see which gives the best accuracy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgZw_uSmOPHk"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ STUDENT TASK #4 ‚Äì Set the optimiser { display-mode:\"form\" }\n",
    "optimiser = \"SGD\"  #@param [\"Adam\", \"RMSprop\", \"SGD\"]\n",
    "\n",
    "import torch.optim as optim, torch\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = {\"Adam\":optim.Adam,\n",
    "       \"RMSprop\":optim.RMSprop,\n",
    "       \"SGD\":optim.SGD}[optimiser](net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# @markdown <details><summary>üí° <b>Which optimiser?</b> (Click to expand)</summary>\n",
    "# @markdown\n",
    "# @markdown Optimisers help Road-Bot adjust its weights after each mistake‚Äîlike picking how to correct your answer after getting a flashcard wrong.\n",
    "# @markdown\n",
    "# @markdown ‚Ä¢ **Adam** ‚Äì Fast, balanced, and usually the best all-around choice for beginners.\n",
    "# @markdown\n",
    "# @markdown &nbsp;&nbsp;&nbsp;&nbsp; ‚ú® Think of it as a smart cruise control that adapts to each curve in the road.\n",
    "# @markdown\n",
    "# @markdown ‚Ä¢ **SGD (Stochastic Gradient Descent)** ‚Äì The original method. Slower, but useful in academic research and fine-tuned projects.\n",
    "# @markdown\n",
    "# @markdown &nbsp;&nbsp;&nbsp;&nbsp; üîß Like shifting gears manually‚Äîmore work, but gives control.\n",
    "# @markdown\n",
    "# @markdown ‚Ä¢ **RMSprop** ‚Äì Similar to Adam, but better for problems with noisy or bumpy data.\n",
    "# @markdown\n",
    "# @markdown &nbsp;&nbsp;&nbsp;&nbsp; üß≠ Helps Road-Bot stay steady on uneven terrain.\n",
    "# @markdown\n",
    "# @markdown\n",
    "# @markdown If you're curious, try the others and compare results!\n",
    "# @markdown </details>\n",
    "\n",
    "\n",
    "\n",
    "# üîí Hidden ‚Äî students just run\n",
    "from tqdm.auto import tqdm\n",
    "def run_epoch(loader, train=True):\n",
    "    net.train() if train else net.eval()\n",
    "    loss_all = correct = 0\n",
    "    desc = \"BATCH-TRAIN\" if train else \"BATCH-VALIDATION\"\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for x,y in tqdm(loader, desc=desc):\n",
    "            if train: opt.zero_grad()\n",
    "            out = net(x); loss = criterion(out,y)\n",
    "            if train: loss.backward(); opt.step()\n",
    "            loss_all += loss.item()*x.size(0)\n",
    "            correct  += (out.argmax(1)==y).sum().item()\n",
    "    return loss_all/len(loader.dataset), correct/len(loader.dataset)\n",
    "\n",
    "hist = {\"tr_loss\":[], \"tr_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "for ep in tqdm(range(EPOCHS), desc=\"EPOCH\"):\n",
    "    tl,ta = run_epoch(train_loader, True)\n",
    "    vl,va = run_epoch(val_loader,   False)\n",
    "    hist[\"tr_loss\"].append(tl); hist[\"tr_acc\"].append(ta)\n",
    "    hist[\"val_loss\"].append(vl); hist[\"val_acc\"].append(va)\n",
    "    print(f\"[{ep+1}/{EPOCHS}] loss:{tl:.3f} acc:{ta:.3f}  |  val_loss:{vl:.3f} val_acc:{va:.3f}\")\n",
    "best_val_acc = max(hist[\"val_acc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vnk0ifHzOSBv"
   },
   "outputs": [],
   "source": [
    "#@title ‚úÖ Quick Check #4 ‚Äì Celebrate results! { display-mode:\"form\" }\n",
    "\n",
    "from IPython.display import Markdown, HTML, display\n",
    "\n",
    "if best_val_acc > 0.80:\n",
    "    # Text summary (1 line)\n",
    "    display(Markdown(f\"<h2 style='color:green'>üéâ ALL DONE! \"\n",
    "                     f\"You reached {best_val_acc:.1%} accuracy.</h2>\"))\n",
    "\n",
    "    # HTML banner\n",
    "    banner_html = \"\"\"\n",
    "    <p style=\"background:#d4edda;\n",
    "              color:#155724;\n",
    "              padding:14px 0;\n",
    "              border-radius:6px;\n",
    "              font-weight:bold;\n",
    "              font-size:1.3em;\n",
    "              text-align:center;\">\n",
    "      üéâ ALL&nbsp;DONE!\n",
    "    </p>\"\"\"\n",
    "    display(HTML(banner_html))\n",
    "else:\n",
    "    print(f\"üîÑ Final validation accuracy {best_val_acc:.1%}. \"\n",
    "          \"Feel free to tweak earlier sliders & re-run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOCDWnNDXX8P"
   },
   "source": [
    "# ‚ú® Reflection Time!\n",
    "1. What surprised you most about teaching Road-Bot?\n",
    "2. If you could teach Road-Bot to recognize anything, what would it be?\n",
    "3. What new word or concept did you learn today?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAqorkPIXbH2"
   },
   "source": [
    "Type your answers in here:\n",
    "\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
